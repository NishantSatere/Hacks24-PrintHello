{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOLjBO4AbhHVFe/AWFX9Wwp"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --user PyPDF2 pypdf langchain==0.0.267 transformers ctransformers requests torch accelerate bitsandbytes chromadb==0.4.6 pdfminer.six==20221105 InstructorEmbedding sentence-transformers faiss-cpu huggingface-hub\n!pip install --user protobuf==3.20.2; sys_platform != 'darwin' protobuf==3.20.2; sys_platform == 'darwin' and platform_machine != 'arm64' protobuf==3.20.3; sys_platform == 'darwin' and platform_machine == 'arm64' auto-gptq==0.2.2 docx2txt unstructured unstructured[pdf] urllib3==1.26.6 accelerate bitsandbytes ; sys_platform != 'win32' bitsandbytes-windows ; sys_platform == 'win32' click flask requests streamlit Streamlit-extras openpyxl\n!pip -qqq install unstructured[pdf]","metadata":{"id":"qZ6P9uHN4a4K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698503250691,"user_tz":-330,"elapsed":156734,"user":{"displayName":"Glen Dsouza","userId":"13902084858020555437"}},"outputId":"e47a50cb-9664-40b2-dfac-33aca746ff04","execution":{"iopub.status.busy":"2023-10-29T05:23:09.403839Z","iopub.execute_input":"2023-10-29T05:23:09.404430Z","iopub.status.idle":"2023-10-29T05:24:59.617280Z","shell.execute_reply.started":"2023-10-29T05:23:09.404400Z","shell.execute_reply":"2023-10-29T05:24:59.615954Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pypdf in /opt/conda/lib/python3.10/site-packages (3.15.5)\nCollecting langchain==0.0.267\n  Downloading langchain-0.0.267-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nCollecting ctransformers\n  Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.31.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.22.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting chromadb==0.4.6\n  Downloading chromadb-0.4.6-py3-none-any.whl (405 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20221105\n  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting InstructorEmbedding\n  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\nCollecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting faiss-cpu\n  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (0.16.4)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.267) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.267) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.267) (3.8.4)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.267) (4.0.2)\nCollecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.267)\n  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\nCollecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.267)\n  Downloading langsmith-0.0.53-py3-none-any.whl (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.267) (2.8.5)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.267) (1.23.5)\nCollecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.267)\n  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.267) (1.10.9)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.267) (8.2.2)\nCollecting chroma-hnswlib==0.7.2 (from chromadb==0.4.6)\n  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6) (0.98.0)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6) (0.22.0)\nCollecting posthog>=2.4.0 (from chromadb==0.4.6)\n  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6) (4.6.3)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.4.6)\n  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.6)\n  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6) (0.13.3)\nCollecting pypika>=0.48.9 (from chromadb==0.4.6)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6) (4.66.1)\nCollecting overrides>=7.3.1 (from chromadb==0.4.6)\n  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6) (5.12.0)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105) (3.1.0)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105) (38.0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from ctransformers) (9.0.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2023.7.22)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (2023.9.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267) (1.3.1)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105) (1.15.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.267) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.267) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.6) (0.27.0)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.6)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.6) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.6) (3.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.6) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.6)\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.6) (2.2.1)\nRequirement already satisfied: python-dateutil>2.1 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.6) (2.8.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.267) (2.0.2)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6) (0.20.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6) (11.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105) (2.21)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6) (3.7.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.267) (1.0.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.6)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6) (1.1.1)\nBuilding wheels for collected packages: chroma-hnswlib, sentence-transformers, pypika\n  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=200798 sha256=978abee4c30ee2f2904e8671ad9dd21e6a549bf87da247050b2624af4a7a5605\n  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=a15428025a56ca8cec3d31627ba29b367c6a60453e9ecbff38b8632404d9c5af\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=770a765fee3f8f7a52b6e5a355b54a3682a629770474b85167e1eb1045656667\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built chroma-hnswlib sentence-transformers pypika\nInstalling collected packages: pypika, monotonic, InstructorEmbedding, faiss-cpu, bitsandbytes, PyPDF2, pulsar-client, overrides, humanfriendly, chroma-hnswlib, posthog, openapi-schema-pydantic, langsmith, coloredlogs, pdfminer.six, onnxruntime, dataclasses-json, ctransformers, sentence-transformers, langchain, chromadb\n\u001b[33m  WARNING: The script humanfriendly is installed in '/root/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/root/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script coloredlogs is installed in '/root/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script onnxruntime_test is installed in '/root/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/root/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed InstructorEmbedding-1.0.1 PyPDF2-3.0.1 bitsandbytes-0.41.1 chroma-hnswlib-0.7.2 chromadb-0.4.6 coloredlogs-15.0.1 ctransformers-0.2.27 dataclasses-json-0.5.14 faiss-cpu-1.7.4 humanfriendly-10.0 langchain-0.0.267 langsmith-0.0.53 monotonic-1.6 onnxruntime-1.16.1 openapi-schema-pydantic-1.2.4 overrides-7.4.0 pdfminer.six-20221105 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 sentence-transformers-2.2.2\nCollecting protobuf==3.20.2\n  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-pubsub 2.17.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\ntensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\ntensorflow-serving-api 2.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.2\n/bin/bash: line 1: sys_platform: command not found\n/bin/bash: line 1: sys_platform: command not found\n/bin/bash: line 1: sys_platform: command not found\n/bin/bash: line 1: sys_platform: command not found\n/bin/bash: line 1: sys_platform: command not found\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-10-29T05:24:59.619125Z","iopub.execute_input":"2023-10-29T05:24:59.619450Z","iopub.status.idle":"2023-10-29T05:25:00.594192Z","shell.execute_reply.started":"2023-10-29T05:24:59.619420Z","shell.execute_reply":"2023-10-29T05:25:00.593198Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Sun Oct 29 05:25:00 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# CONSTANTS\n\nimport os\nfrom chromadb.config import Settings\nfrom langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader\nfrom langchain.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader\n\nSOURCE_DIRECTORY = \"../input/newdata/data\"\nPERSIST_DIRECTORY = \"\"\n\nMODELS_PATH = \"./models\"\n\nINGEST_THREADS = 2\nCHROMA_SETTINGS = Settings(\n    anonymized_telemetry=False,\n    is_persistent=True,\n)\n\n# Context Window and Max New Tokens\nCONTEXT_WINDOW_SIZE = 2048\nMAX_NEW_TOKENS = CONTEXT_WINDOW_SIZE  # int(CONTEXT_WINDOW_SIZE/4)\n\nN_GPU_LAYERS = 60\nN_BATCH = 512\n\nDOCUMENT_MAP = {\n    \".txt\": TextLoader,\n    \".md\": UnstructuredMarkdownLoader,\n    \".py\": TextLoader,\n    # \".pdf\": PDFMinerLoader,\n    \".pdf\": UnstructuredFileLoader,\n    \".csv\": CSVLoader,\n    \".xls\": UnstructuredExcelLoader,\n    \".xlsx\": UnstructuredExcelLoader,\n    \".docx\": Docx2txtLoader,\n    \".doc\": Docx2txtLoader,\n}\n\nEMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"  # Uses 1.5 GB of VRAM (High Accuracy with lower VRAM usage)\n# EMBEDDING_MODEL_NAME = \"hkunlp/instructor-xl\" # Uses 5 GB of VRAM (Most Accurate of all models)\n\n\nMODEL_ID = \"TheBloke/zephyr-7B-alpha-GGUF\"\nMODEL_BASENAME = \"zephyr-7b-alpha.Q2_K.gguf\"\n\n# MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n# MODEL_BASENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n\ndevice_type = \"cuda\"\nmodel_type = \"mistral\"","metadata":{"id":"7sz1yijwGonq","executionInfo":{"status":"ok","timestamp":1698503253871,"user_tz":-330,"elapsed":3193,"user":{"displayName":"Glen Dsouza","userId":"13902084858020555437"}},"execution":{"iopub.status.busy":"2023-10-29T05:25:00.595584Z","iopub.execute_input":"2023-10-29T05:25:00.595890Z","iopub.status.idle":"2023-10-29T05:25:04.213867Z","shell.execute_reply.started":"2023-10-29T05:25:00.595862Z","shell.execute_reply":"2023-10-29T05:25:04.213008Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#INGEST\n\nimport os\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\nimport click\nimport torch\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.text_splitter import Language, RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nimport logging\n\ndef file_log(logentry):\n   file1 = open(\"file_ingest.log\",\"a\")\n   file1.write(logentry + \"\\n\")\n   file1.close()\n   print(logentry + \"\\n\")\n\ndef load_single_document(file_path: str) -> Document:\n    # Loads a single document from a file path\n    try:\n       file_extension = os.path.splitext(file_path)[1]\n       loader_class = DOCUMENT_MAP.get(file_extension)\n       if loader_class:\n           file_log(file_path + ' loaded.')\n           loader = loader_class(file_path)\n       else:\n           file_log(file_path + ' document type is undefined.')\n           raise ValueError(\"Document type is undefined\")\n       return loader.load()[0]\n    except Exception as ex:\n       file_log('%s loading error: \\n%s' % (file_path, ex))\n       return None\n\n\ndef load_document_batch(filepaths):\n    logging.info(\"Loading document batch\")\n    # create a thread pool\n    with ThreadPoolExecutor(len(filepaths)) as exe:\n        # load files\n        futures = [exe.submit(load_single_document, name) for name in filepaths]\n        # collect data\n        data_list = [future.result() for future in futures]\n        # return data and file paths\n        return (data_list, filepaths)\n\ndef split_documents(documents: list[Document]) -> tuple[list[Document], list[Document]]:\n    # Splits documents for correct Text Splitter\n    text_docs, python_docs = [], []\n    for doc in documents:\n        if doc is not None:\n           file_extension = os.path.splitext(doc.metadata[\"source\"])[1]\n           if file_extension == \".py\":\n               python_docs.append(doc)\n           else:\n               text_docs.append(doc)\n    return text_docs, python_docs\n\ndef load_documents(source_dir: str) -> list[Document]:\n    # Loads all documents from the source documents directory, including nested folders\n    paths = []\n    for root, _, files in os.walk(source_dir):\n        for file_name in files:\n            print(\"file_name\")\n            print('Importing: ' + file_name)\n            file_extension = os.path.splitext(file_name)[1]\n            source_file_path = os.path.join(root, file_name)\n            if file_extension in DOCUMENT_MAP.keys():\n                paths.append(source_file_path)\n\n    # Have at least one worker and at most INGEST_THREADS workers\n    n_workers = min(INGEST_THREADS, max(len(paths), 1))\n    chunksize = round(len(paths) / n_workers)\n    docs = []\n    with ProcessPoolExecutor(n_workers) as executor:\n        futures = []\n        # split the load operations into chunks\n        for i in range(0, len(paths), chunksize):\n            # select a chunk of filenames\n            filepaths = paths[i : (i + chunksize)]\n            # submit the task\n            try:\n               future = executor.submit(load_document_batch, filepaths)\n            except Exception as ex:\n               file_log('executor task failed: %s' % (ex))\n               future = None\n            if future is not None:\n               futures.append(future)\n        # process all results\n        for future in as_completed(futures):\n            # open the file and load the data\n            try:\n                contents, _ = future.result()\n                docs.extend(contents)\n            except Exception as ex:\n                file_log('Exception: %s' % (ex))\n\n    return docs\n\ndef ingest(device_type=\"cuda\"):\n    # Load documents and split in chunks\n    documents = load_documents(SOURCE_DIRECTORY)\n    text_documents, python_documents = split_documents(documents)\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n    python_splitter = RecursiveCharacterTextSplitter.from_language(\n        language=Language.PYTHON, chunk_size=880, chunk_overlap=200\n    )\n    texts = text_splitter.split_documents(text_documents)\n    texts.extend(python_splitter.split_documents(python_documents))\n\n    embeddings = HuggingFaceInstructEmbeddings(\n        model_name=EMBEDDING_MODEL_NAME,\n        model_kwargs={\"device\": device_type},\n    )\n    db = Chroma.from_documents(\n        texts,\n        embeddings,\n        persist_directory=PERSIST_DIRECTORY,\n        client_settings=CHROMA_SETTINGS,\n    )\n\ningest()\n","metadata":{"id":"ipTg8LRLTfwg","executionInfo":{"status":"ok","timestamp":1698503273802,"user_tz":-330,"elapsed":6659,"user":{"displayName":"Glen Dsouza","userId":"13902084858020555437"}},"execution":{"iopub.status.busy":"2023-10-29T05:25:04.216087Z","iopub.execute_input":"2023-10-29T05:25:04.216512Z","iopub.status.idle":"2023-10-29T05:25:43.611889Z","shell.execute_reply.started":"2023-10-29T05:25:04.216485Z","shell.execute_reply":"2023-10-29T05:25:43.610806Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"file_name\nImporting: AFFAIREC.P.ETM.N.c.FRANCE.pdf\n../input/newdata/data/AFFAIREC.P.ETM.N.c.FRANCE.pdf loaded.\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/root/.local/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import trange\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)c7233/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9da5e3d94f1e4a0e8f8ef1d9e8f37876"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fc6b8497f1245b8a1886af5c863f29e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"808c06fd776f473a97f61928fac4b5bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0587252eb1b645fd86181c4e3f637055"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)9fb15c7233/README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a99284db7e82447dae0d2fa6127766dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)b15c7233/config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f196c9828514ad894f360ade584cca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4dc992055554f4f944a201668cbef36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f41c299ffb394f20ac4cc960bcaffbe5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70072c0ad2fa4a60b4e50093234abdd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a83b2e81b14dca8244bdef4352f33e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0425842f0e9b446ba079e4dda534b730"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)c7233/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89adb14c44ea4182b1e834fc010e5104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0e8e4b1f79540f0b90d6e782816b425"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)15c7233/modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47776a5970e94873a626b7c5292a129c"}},"metadata":{}},{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"code","source":"embeddings = HuggingFaceInstructEmbeddings(\n        model_name=EMBEDDING_MODEL_NAME,\n        model_kwargs={\"device\": 'cuda'},\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VL0PXBqDk3Hj","executionInfo":{"status":"ok","timestamp":1698503306275,"user_tz":-330,"elapsed":15028,"user":{"displayName":"Glen Dsouza","userId":"13902084858020555437"}},"outputId":"091db8a9-dadd-479f-cfc0-0293810fe031","execution":{"iopub.status.busy":"2023-10-29T05:25:43.613560Z","iopub.execute_input":"2023-10-29T05:25:43.614120Z","iopub.status.idle":"2023-10-29T05:25:47.630150Z","shell.execute_reply.started":"2023-10-29T05:25:43.614086Z","shell.execute_reply":"2023-10-29T05:25:47.629214Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"code","source":"CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" \nFORCE_CMAKE=1 \n!pip install llama-cpp-python==0.1.83 --no-cache-dir\n!pip install -qqq auto_gptq","metadata":{"execution":{"iopub.status.busy":"2023-10-29T05:25:47.631383Z","iopub.execute_input":"2023-10-29T05:25:47.631677Z","iopub.status.idle":"2023-10-29T05:26:52.799974Z","shell.execute_reply.started":"2023-10-29T05:25:47.631650Z","shell.execute_reply":"2023-10-29T05:26:52.798950Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting llama-cpp-python==0.1.83\n  Downloading llama_cpp_python-0.1.83.tar.gz (1.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.1.83) (4.6.3)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.1.83) (1.23.5)\nCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.83)\n  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.83-cp310-cp310-linux_x86_64.whl size=408771 sha256=112ba586d61b5785da401bae792e0beb6b67e49d21f6b740efb643c15b54ad2d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ntvr8ft5/wheels/3f/39/6f/3e75230ce84bb465df194bca6c0c7b936dc4b0b3c83389688d\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.83\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"#LOAD MODEL\nimport torch\nfrom auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.llms import LlamaCpp\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n)\n\ndef load_quantized_model_gguf_ggml(model_id, model_basename, device_type, logging):\n    try:\n        model_path = hf_hub_download(\n            repo_id=model_id,\n            filename=model_basename,\n            resume_download=True,\n            cache_dir=MODELS_PATH,\n        )\n        kwargs = {\n            \"model_path\": model_path,\n            \"n_ctx\": CONTEXT_WINDOW_SIZE,\n            \"max_tokens\": MAX_NEW_TOKENS,\n            \"n_batch\": N_BATCH,  # set this based on your GPU & CPU RAM\n        }\n        kwargs[\"n_gpu_layers\"] = N_GPU_LAYERS  # set this based on your GPU\n\n        return LlamaCpp(**kwargs)\n    except Exception as e:\n        print(e)\n        if \"ggml\" in model_basename:\n            logging.INFO(\"If you were using GGML model, LLAMA-CPP Dropped Support, Use GGUF Instead\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2023-10-29T05:26:52.801705Z","iopub.execute_input":"2023-10-29T05:26:52.802485Z","iopub.status.idle":"2023-10-29T05:26:53.170524Z","shell.execute_reply.started":"2023-10-29T05:26:52.802443Z","shell.execute_reply":"2023-10-29T05:26:53.169578Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#PROMPT TEMPLATES\n\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\n\n\nsystem_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\nRead the given context before answering questions and think step by step. If you can not answer a user question based on \nthe provided context, inform the user. Do not use any other information for answering user other than provided context.\"\"\"\n\n\ndef get_prompt_template(system_prompt, history=False):\n    B_INST, E_INST = \"<s>[INST] \", \" [/INST]\"\n    if history:\n        prompt_template = (\n            B_INST\n            + system_prompt\n            + \"\"\"\n\n        Context: {history} \\n {context}\n        User: {question}\"\"\"\n            + E_INST\n        )\n        prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=prompt_template)\n    else:\n        prompt_template = (\n            B_INST\n            + system_prompt\n            + \"\"\"\n\n        Context: {context}\n        User: {question}\"\"\"\n            + E_INST\n        )\n        prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n    \n    memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\n\n    return (\n        prompt,\n        memory,\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-29T05:26:53.171655Z","iopub.execute_input":"2023-10-29T05:26:53.171979Z","iopub.status.idle":"2023-10-29T05:26:53.179692Z","shell.execute_reply.started":"2023-10-29T05:26:53.171940Z","shell.execute_reply":"2023-10-29T05:26:53.178708Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#MAIN CODE\n\nimport os\nimport logging\nimport click\nimport torch\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response\nfrom langchain.callbacks.manager import CallbackManager\n\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    GenerationConfig,\n    pipeline,\n)\n\ndef retrieval_qa_pipline(device_type=\"cuda\", use_history=True, promptTemplate_type=\"mistral\"):\n    \n    embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cuda\"})\n    db = Chroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS\n    )\n    retriever = db.as_retriever()\n\n    prompt, memory = get_prompt_template(promptTemplate_type, history=use_history)\n\n    llm = load_quantized_model_gguf_ggml(MODEL_ID, MODEL_BASENAME, \"cuda\", logging)\n    \n    if use_history:\n        qa = RetrievalQA.from_chain_type(\n            llm=llm,\n            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n            retriever=retriever,\n            return_source_documents=True,  # verbose=True,\n            callbacks=callback_manager,\n            chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n        )\n    else:\n        qa = RetrievalQA.from_chain_type(\n            llm=llm,\n            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n            retriever=retriever,\n            return_source_documents=True,  # verbose=True,\n            callbacks=callback_manager,\n            chain_type_kwargs={\n                \"prompt\": prompt,\n            },\n        )\n\n    return qa\n\ndef load_qa():\n    if not os.path.exists(MODELS_PATH):\n        os.mkdir(MODELS_PATH)\n\n    qa = retrieval_qa_pipline()\n    return qa\n\npublic_qa = load_qa()\n\ndef chat(query, device_type=\"cuda\", show_sources=True, use_history=True, model_type=\"mistral\"):    \n    query = input(\"\\nEnter a query: \")\n    res = public_qa(query)\n    answer, docs = res[\"result\"], res[\"source_documents\"]\n\n    # Print the result\n    print(\"\\n\\n> Question:\")\n    print(query)\n    print(\"\\n> Answer:\")\n    print(answer)\n    \n    metadata = [] \n    for document in docs:\n        metadata.append(document.metadata[\"source\"]) \n\n    return {\"result\":answer, \"source\": metadata }\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T05:26:53.180948Z","iopub.execute_input":"2023-10-29T05:26:53.181635Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)r-7b-alpha.Q2_K.gguf:   0%|          | 0.00/3.08G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"303da2728d3f449b98f8d4d3de744ff3"}},"metadata":{}}]},{"cell_type":"code","source":"from fastapi import FastAPI, UploadFile, File\nfrom pydantic import BaseModel\nimport json\nimport uvicorn\nfrom pyngrok import ngrok\nfrom fastapi.middleware.cors import CORSMiddleware\nimport nest_asyncio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app = FastAPI()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"origins = [\"*\"]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@app.post(\"/qa\")\ndef process_question(request: QuestionRequest):\n    question = request.question\n    result = chat(question)\n    return result\n\n@app.post(\"/ingest\")\nasync def upload_file(file: UploadFile = File(...)):\n    try:\n        f_path = f\"{BASE_DIR}/docs\"\n        file_path = os.path.join(f_path, file.filename)\n        with open(file_path, \"wb\") as pdf_file:\n            pdf_file.write(file.file.read())\n        ingest()\n        return JSONResponse(content={\"message\": \"File uploaded successfully\"}, status_code=200)\n    except Exception as e:\n        return JSONResponse(content={\"message\": f\"Failed to upload file: {str(e)}\"}, status_code=500)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ngrok authtoken 2XJVgzMSOLEDtFaLnu73L4KLbwh_2nfCsTZMzH8UurX1LAGRd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698379317441,"user":{"displayName":"Glen Dsouza","userId":"13902084858020555437"},"user_tz":-330},"id":"hfttv6qWZIIN","outputId":"4a95b9f7-b2b3-4edc-f6a9-e3ec3cf588d2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ngrok_tunnel = ngrok.connect(8000)\nprint('Public URL:', ngrok_tunnel.public_url)\nnest_asyncio.apply()\nuvicorn.run(app, port=8000)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":453,"status":"ok","timestamp":1698379324784,"user":{"displayName":"Glen Dsouza","userId":"13902084858020555437"},"user_tz":-330},"id":"uJFpGDRpAt0n","outputId":"a1854806-555d-475e-d933-658d20da9224","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"fPa4yUpbJfzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}